"""
This python program scans the data generated by Archi and makes a graph of the file history of Hadoop
"""

import csv
from os import path, getcwd, listdir
import pandas as pd

results_directory = path.join(getcwd(), 'results')
filenames = listdir(results_directory)  # get all files' and folders' names in the current directory

commits = []
for filename in filenames:  # loop through all the files and folders
    # check whether the current object is a folder or not
    if path.isdir(path.join(path.abspath(results_directory), filename)):
        commits.append([int(filename.split(sep='_')[0]), filename.split(sep='_')[1]])

commits = sorted(commits)

results = []
for directory in commits:
    commit_nr = directory[0]
    commit_hash = directory[1]

    df = pd.read_csv(results_directory + '\\' + str(commit_nr) + '_' + commit_hash + r'\AllTactics.csv')
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    df = df.loc[:, ~df.columns.str.contains('File')]
    df = df.loc[:, ~df.columns.str.contains('Id')]

    results.append(df.shape[0])


f = open('results_sizes.csv', 'w')
writer = csv.writer(f)
writer.writerow(results)
f.close()
